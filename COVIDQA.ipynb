{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2caa758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sent2vec\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sentence_splitter import SentenceSplitter \n",
    "from sentence_splitter import split_text_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5470c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "biosentvec = sent2vec.Sent2vecModel()\n",
    "biosentvec.load_model('bio_sent_vec.file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "data = json.load(open('dataset/covidqa.json'))\n",
    "for category in data['categories']:\n",
    "    for subcategory in category['sub_categories']:\n",
    "        for answer in subcategory['answers']:\n",
    "            df.append({\n",
    "                'natural_language_query': subcategory['nq_name'],\n",
    "                'keyword_query': subcategory['kq_name'],\n",
    "                'cord_id': answer['id'],\n",
    "                'title': answer['title'],\n",
    "                'answer': answer['exact_answer']\n",
    "            })\n",
    "df = pd.DataFrame(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b89731",
   "metadata": {},
   "outputs": [],
   "source": [
    "cord_ids = set(df['cord_id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('dataset/cord19-round1/metadata.csv')\n",
    "display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81648ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    return np.sum(a * b) / ((np.sum(a ** 2) ** 0.5) * (np.sum(b ** 2) ** 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = {}\n",
    "body = defaultdict(list)\n",
    "\n",
    "splitter = SentenceSplitter(language = 'en')\n",
    "\n",
    "for ind, row in tqdm.tqdm(metadata.iterrows()):\n",
    "    \n",
    "    if row['cord_uid'] in cord_ids:\n",
    "        \n",
    "        text = ''\n",
    "        \n",
    "        for filename in row['sha'].split('; '):\n",
    "        \n",
    "            filename = filename + '.json'\n",
    "            pdf_path = '/'.join(['dataset', 'cord19-round1', row['full_text_file'], 'pdf_json', filename])\n",
    "            path[row['cord_uid']] = pdf_path\n",
    "\n",
    "            file = json.load(open(pdf_path))\n",
    "\n",
    "            for abstract_content in file['abstract']:\n",
    "                text += ' ' + abstract_content['text']\n",
    "\n",
    "            for body_content in file['body_text']:\n",
    "                text += ' ' + body_content['text']\n",
    "                \n",
    "            for ref_content in file['ref_entries'].values():\n",
    "                text += ' ' + ref_content['text']\n",
    "\n",
    "        sentences = splitter.split(text = text)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            body[row['cord_uid']].append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ef909",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOP_WORDS)\n",
    "\n",
    "custom_stop_words = [\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI','table'\n",
    "]\n",
    "\n",
    "stopwords |= set(custom_stop_words)\n",
    "\n",
    "stopwords = set([word.lower() for word in stopwords])\n",
    "\n",
    "def clean(text):\n",
    "    text = text.split()\n",
    "    text = ' '.join(text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_text = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in stopwords:\n",
    "            filtered_text.append(word)\n",
    "    filtered_text = ' '.join(filtered_text)\n",
    "    return filtered_text\n",
    "\n",
    "def further_preprocess(sent):\n",
    "    sent = clean(sent)\n",
    "    sent = re.sub('[^a-z ]+', '', sent)\n",
    "    sent = clean(sent)\n",
    "    sent = remove_stopwords(sent)\n",
    "    sent = clean(sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr = 0\n",
    "pat1 = 0\n",
    "rat3 = 0\n",
    "n = df.shape[0]\n",
    "\n",
    "for index, row in tqdm.tqdm(list(df.iterrows())[:n]):\n",
    "    \n",
    "    answer = clean(row['answer'])\n",
    "    query = clean(row['natural_language_query'])\n",
    "    query = further_preprocess(query)\n",
    "    \n",
    "    correct = set()\n",
    "    for ind, sent in enumerate(body[row['cord_id']]):\n",
    "        sent = clean(sent)\n",
    "        if sent.find(answer) != -1:\n",
    "            correct.add(ind)\n",
    "\n",
    "    sents = []\n",
    "    tokenized_sents = []\n",
    "    for sent in body[row['cord_id']]:\n",
    "        sent = clean(sent)\n",
    "        sents.append(sent)\n",
    "        sent = further_preprocess(sent)\n",
    "        tokenized_sents.append(sent.split())\n",
    "        \n",
    "    bm25 = BM25Okapi(tokenized_sents)\n",
    "    bm_scores = bm25.get_scores(query.split())\n",
    "    bm_scores = np.array(bm_scores)\n",
    "    bm_scores /= np.sum(bm_scores ** 2) ** 0.5\n",
    "    \n",
    "    query_emb = biosentvec.embed_sentences([query])[0]\n",
    "    bioemb = biosentvec.embed_sentences(sents)\n",
    "    bioemb_scores = []\n",
    "    for i in range(len(sents)):\n",
    "        if np.sum(bioemb[i]):\n",
    "            bioemb_scores.append(cosine(query_emb, bioemb[i]))\n",
    "        else:\n",
    "            bioemb_scores.append(0)\n",
    "    bioemb_scores = np.array(bioemb_scores)\n",
    "    bioemb_scores /= np.sum(bioemb_scores ** 2) ** 0.5\n",
    "    \n",
    "    edge_count = 0\n",
    "    adjList = defaultdict(set)\n",
    "    for i in range(len(sents)):\n",
    "        for j in range(i + 1, len(sents)):\n",
    "            if np.sum(bioemb[i]) and np.sum(bioemb[j]) and cosine(bioemb[i], bioemb[j]) >= 0.1:\n",
    "                if cosine(bioemb[i], query_emb) >= 0.1 and cosine(bioemb[j], query_emb) >= 0.1:\n",
    "                    adjList[i].add(j)\n",
    "                    adjList[j].add(i)\n",
    "                    edge_count += 1\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list(range(len(sents))))\n",
    "\n",
    "    for i in adjList.keys():\n",
    "        for j in adjList[i]:\n",
    "            G.add_edge(i, j)\n",
    "\n",
    "    pagerank = nx.pagerank(G)\n",
    "    pr_scores = [0] * len(sents)\n",
    "    for key, val in pagerank.items():\n",
    "        pr_scores[key] = val\n",
    "    pr_scores = np.array(pr_scores)\n",
    "    pr_scores /= np.sum(pr_scores ** 2) ** 0.5\n",
    "    \n",
    "    tmp = []\n",
    "    for i in range(len(sents)):\n",
    "        tmp.append([bm_scores[i] + bioemb_scores[i] + pr_scores[i], i])\n",
    "    tmp.sort(reverse = True)\n",
    "    \n",
    "    ranklist = []\n",
    "    for entry in tmp:\n",
    "        ranklist.append(entry[1])\n",
    "    \n",
    "    if ranklist[0] in correct:\n",
    "        pat1 += 1\n",
    "        \n",
    "    rat3 +=  len(set(ranklist[: 3]) & correct) / len(correct)\n",
    "    \n",
    "    rr = []\n",
    "    for i in correct:\n",
    "        rr.append(1 / (ranklist.index(i) + 1))\n",
    "    mrr += max(rr)\n",
    "\n",
    "pat1 /= n\n",
    "rat3 /= n\n",
    "mrr /= n\n",
    "\n",
    "print('P@1: %.4f' % round(pat1, 4))\n",
    "print('R@3: %.4f' % round(rat3, 4))\n",
    "print('MRR: %.4f' % round(mrr, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
